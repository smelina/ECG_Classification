{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小数据brugada(一维)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p578240-1.csv', 'p578240-10.csv', 'p578240-2.csv', 'p578240-3.csv', 'p578240-4.csv', 'p578240-5.csv', 'p578240-6.csv', 'p578240-7.csv', 'p578240-8.csv', 'p578240-9.csv', 'p663312-1.csv', 'p663312-10.csv', 'p663312-2.csv', 'p663312-3.csv', 'p663312-4.csv', 'p663312-5.csv', 'p663312-6.csv', 'p663312-7.csv', 'p663312-8.csv', 'p663312-9.csv', 'p769439-1.csv', 'p769439-10.csv', 'p769439-2.csv', 'p769439-3.csv', 'p769439-4.csv', 'p769439-5.csv', 'p769439-6.csv', 'p769439-7.csv', 'p769439-8.csv', 'p769439-9.csv', 'p825958-1.csv', 'p825958-10.csv', 'p825958-2.csv', 'p825958-3.csv', 'p825958-4.csv', 'p825958-5.csv', 'p825958-6.csv', 'p825958-7.csv', 'p825958-8.csv', 'p825958-9.csv', 'p848694-1.csv', 'p848694-10.csv', 'p848694-2.csv', 'p848694-3.csv', 'p848694-4.csv', 'p848694-5.csv', 'p848694-6.csv', 'p848694-7.csv', 'p848694-8.csv', 'p848694-9.csv', 'p849710-1.csv', 'p849710-10.csv', 'p849710-2.csv', 'p849710-3.csv', 'p849710-4.csv', 'p849710-5.csv', 'p849710-6.csv', 'p849710-7.csv', 'p849710-8.csv', 'p849710-9.csv', 'p864195-1.csv', 'p864195-10.csv', 'p864195-2.csv', 'p864195-3.csv', 'p864195-4.csv', 'p864195-5.csv', 'p864195-6.csv', 'p864195-7.csv', 'p864195-8.csv', 'p864195-9.csv', 'p898046-1.csv', 'p898046-10.csv', 'p898046-2.csv', 'p898046-3.csv', 'p898046-4.csv', 'p898046-5.csv', 'p898046-6.csv', 'p898046-7.csv', 'p898046-8.csv', 'p898046-9.csv', 'p902980-1.csv', 'p902980-10.csv', 'p902980-2.csv', 'p902980-3.csv', 'p902980-4.csv', 'p902980-5.csv', 'p902980-6.csv', 'p902980-7.csv', 'p902980-8.csv', 'p902980-9.csv', 'p903818-1.csv', 'p903818-10.csv', 'p903818-2.csv', 'p903818-3.csv', 'p903818-4.csv', 'p903818-5.csv', 'p903818-6.csv', 'p903818-7.csv', 'p903818-8.csv', 'p903818-9.csv', 'p951805-1.csv', 'p951805-10.csv', 'p951805-2.csv', 'p951805-3.csv', 'p951805-4.csv', 'p951805-5.csv', 'p951805-6.csv', 'p951805-7.csv', 'p951805-8.csv', 'p951805-9.csv']\n",
      "['p130527-1.csv', 'p130527-10.csv', 'p130527-2.csv', 'p130527-3.csv', 'p130527-4.csv', 'p130527-5.csv', 'p130527-6.csv', 'p130527-7.csv', 'p130527-8.csv', 'p130527-9.csv', 'p138171-1.csv', 'p138171-10.csv', 'p138171-2.csv', 'p138171-3.csv', 'p138171-4.csv', 'p138171-5.csv', 'p138171-6.csv', 'p138171-7.csv', 'p138171-8.csv', 'p138171-9.csv', 'p555665-1.csv', 'p555665-10.csv', 'p555665-2.csv', 'p555665-3.csv', 'p555665-4.csv', 'p555665-5.csv', 'p555665-6.csv', 'p555665-7.csv', 'p555665-8.csv', 'p555665-9.csv', 'p946317-1.csv', 'p946317-10.csv', 'p946317-2.csv', 'p946317-3.csv', 'p946317-4.csv', 'p946317-5.csv', 'p946317-6.csv', 'p946317-7.csv', 'p946317-8.csv', 'p946317-9.csv', 'p953346-1.csv', 'p953346-10.csv', 'p953346-2.csv', 'p953346-3.csv', 'p953346-4.csv', 'p953346-5.csv', 'p953346-6.csv', 'p953346-7.csv', 'p953346-8.csv', 'p953346-9.csv', 'p973399-1.csv', 'p973399-10.csv', 'p973399-2.csv', 'p973399-3.csv', 'p973399-4.csv', 'p973399-5.csv', 'p973399-6.csv', 'p973399-7.csv', 'p973399-8.csv', 'p973399-9.csv', 'p977058-1.csv', 'p977058-10.csv', 'p977058-2.csv', 'p977058-3.csv', 'p977058-4.csv', 'p977058-5.csv', 'p977058-6.csv', 'p977058-7.csv', 'p977058-8.csv', 'p977058-9.csv', 'p977151-1.csv', 'p977151-10.csv', 'p977151-2.csv', 'p977151-3.csv', 'p977151-4.csv', 'p977151-5.csv', 'p977151-6.csv', 'p977151-7.csv', 'p977151-8.csv', 'p977151-9.csv', 'p977299-1.csv', 'p977299-10.csv', 'p977299-2.csv', 'p977299-3.csv', 'p977299-4.csv', 'p977299-5.csv', 'p977299-6.csv', 'p977299-7.csv', 'p977299-8.csv', 'p977299-9.csv', 'p977417-1.csv', 'p977417-10.csv', 'p977417-2.csv', 'p977417-3.csv', 'p977417-4.csv', 'p977417-5.csv', 'p977417-6.csv', 'p977417-7.csv', 'p977417-8.csv', 'p977417-9.csv']\n",
      "['伦宝荣P906780-1.csv', '伦宝荣P906780-10.csv', '伦宝荣P906780-2.csv', '伦宝荣P906780-3.csv', '伦宝荣P906780-4.csv', '伦宝荣P906780-5.csv', '伦宝荣P906780-6.csv', '伦宝荣P906780-7.csv', '伦宝荣P906780-8.csv', '伦宝荣P906780-9.csv', '倪桦P906611-1.csv', '倪桦P906611-10.csv', '倪桦P906611-2.csv', '倪桦P906611-3.csv', '倪桦P906611-4.csv', '倪桦P906611-5.csv', '倪桦P906611-6.csv', '倪桦P906611-7.csv', '倪桦P906611-8.csv', '倪桦P906611-9.csv', '吕喜妹p907611-1.csv', '吕喜妹p907611-10.csv', '吕喜妹p907611-2.csv', '吕喜妹p907611-3.csv', '吕喜妹p907611-4.csv', '吕喜妹p907611-5.csv', '吕喜妹p907611-6.csv', '吕喜妹p907611-7.csv', '吕喜妹p907611-8.csv', '吕喜妹p907611-9.csv', '吕学燕P847571-1.csv', '吕学燕P847571-10.csv', '吕学燕P847571-2.csv', '吕学燕P847571-3.csv', '吕学燕P847571-4.csv', '吕学燕P847571-5.csv', '吕学燕P847571-6.csv', '吕学燕P847571-7.csv', '吕学燕P847571-8.csv', '吕学燕P847571-9.csv', '吕展尧P882898-1.csv', '吕展尧P882898-10.csv', '吕展尧P882898-2.csv', '吕展尧P882898-3.csv', '吕展尧P882898-4.csv', '吕展尧P882898-5.csv', '吕展尧P882898-6.csv', '吕展尧P882898-7.csv', '吕展尧P882898-8.csv', '吕展尧P882898-9.csv', '栾栋P435757-1.csv', '栾栋P435757-10.csv', '栾栋P435757-2.csv', '栾栋P435757-3.csv', '栾栋P435757-4.csv', '栾栋P435757-5.csv', '栾栋P435757-6.csv', '栾栋P435757-7.csv', '栾栋P435757-8.csv', '栾栋P435757-9.csv', '欧丽容P888153-1.csv', '欧丽容P888153-10.csv', '欧丽容P888153-2.csv', '欧丽容P888153-3.csv', '欧丽容P888153-4.csv', '欧丽容P888153-5.csv', '欧丽容P888153-6.csv', '欧丽容P888153-7.csv', '欧丽容P888153-8.csv', '欧丽容P888153-9.csv', '欧梅娇P904483-1.csv', '欧梅娇P904483-10.csv', '欧梅娇P904483-2.csv', '欧梅娇P904483-3.csv', '欧梅娇P904483-4.csv', '欧梅娇P904483-5.csv', '欧梅娇P904483-6.csv', '欧梅娇P904483-7.csv', '欧梅娇P904483-8.csv', '欧梅娇P904483-9.csv', '欧静海G11776-1.csv', '欧静海G11776-10.csv', '欧静海G11776-2.csv', '欧静海G11776-3.csv', '欧静海G11776-4.csv', '欧静海G11776-5.csv', '欧静海G11776-6.csv', '欧静海G11776-7.csv', '欧静海G11776-8.csv', '欧静海G11776-9.csv', '罗东权P904821-1.csv', '罗东权P904821-10.csv', '罗东权P904821-2.csv', '罗东权P904821-3.csv', '罗东权P904821-4.csv', '罗东权P904821-5.csv', '罗东权P904821-6.csv', '罗东权P904821-7.csv', '罗东权P904821-8.csv', '罗东权P904821-9.csv', '罗丽芳P734748-1.csv', '罗丽芳P734748-10.csv', '罗丽芳P734748-2.csv', '罗丽芳P734748-3.csv', '罗丽芳P734748-4.csv', '罗丽芳P734748-5.csv', '罗丽芳P734748-6.csv', '罗丽芳P734748-7.csv', '罗丽芳P734748-8.csv', '罗丽芳P734748-9.csv', '罗向P895903-1.csv', '罗向P895903-10.csv', '罗向P895903-2.csv', '罗向P895903-3.csv', '罗向P895903-4.csv', '罗向P895903-5.csv', '罗向P895903-6.csv', '罗向P895903-7.csv', '罗向P895903-8.csv', '罗向P895903-9.csv', '罗定新P482879-1.csv', '罗定新P482879-10.csv', '罗定新P482879-2.csv', '罗定新P482879-3.csv', '罗定新P482879-4.csv', '罗定新P482879-5.csv', '罗定新P482879-6.csv', '罗定新P482879-7.csv', '罗定新P482879-8.csv', '罗定新P482879-9.csv', '罗惜珠P907667-1.csv', '罗惜珠P907667-10.csv', '罗惜珠P907667-2.csv', '罗惜珠P907667-3.csv', '罗惜珠P907667-4.csv', '罗惜珠P907667-5.csv', '罗惜珠P907667-6.csv', '罗惜珠P907667-7.csv', '罗惜珠P907667-8.csv', '罗惜珠P907667-9.csv', '罗桂兰p165562-1.csv', '罗桂兰p165562-10.csv', '罗桂兰p165562-2.csv', '罗桂兰p165562-3.csv', '罗桂兰p165562-4.csv', '罗桂兰p165562-5.csv', '罗桂兰p165562-6.csv', '罗桂兰p165562-7.csv', '罗桂兰p165562-8.csv', '罗桂兰p165562-9.csv', '罗森月P907740-1.csv', '罗森月P907740-10.csv', '罗森月P907740-2.csv', '罗森月P907740-3.csv', '罗森月P907740-4.csv', '罗森月P907740-5.csv', '罗森月P907740-6.csv', '罗森月P907740-7.csv', '罗森月P907740-8.csv', '罗森月P907740-9.csv', '罗润伯G7420-1.csv', '罗润伯G7420-10.csv', '罗润伯G7420-2.csv', '罗润伯G7420-3.csv', '罗润伯G7420-4.csv', '罗润伯G7420-5.csv', '罗润伯G7420-6.csv', '罗润伯G7420-7.csv', '罗润伯G7420-8.csv', '罗润伯G7420-9.csv', '罗辉云P903823-1.csv', '罗辉云P903823-10.csv', '罗辉云P903823-2.csv', '罗辉云P903823-3.csv', '罗辉云P903823-4.csv', '罗辉云P903823-5.csv', '罗辉云P903823-6.csv', '罗辉云P903823-7.csv', '罗辉云P903823-8.csv', '罗辉云P903823-9.csv', '罗雨枚P902944-1.csv', '罗雨枚P902944-10.csv', '罗雨枚P902944-2.csv', '罗雨枚P902944-3.csv', '罗雨枚P902944-4.csv', '罗雨枚P902944-5.csv', '罗雨枚P902944-6.csv', '罗雨枚P902944-7.csv', '罗雨枚P902944-8.csv', '罗雨枚P902944-9.csv', '莫俊斌P907550-1.csv', '莫俊斌P907550-10.csv', '莫俊斌P907550-2.csv', '莫俊斌P907550-3.csv', '莫俊斌P907550-4.csv', '莫俊斌P907550-5.csv', '莫俊斌P907550-6.csv', '莫俊斌P907550-7.csv', '莫俊斌P907550-8.csv', '莫俊斌P907550-9.csv', '莫建华P904808-1.csv', '莫建华P904808-10.csv', '莫建华P904808-2.csv', '莫建华P904808-3.csv', '莫建华P904808-4.csv', '莫建华P904808-5.csv', '莫建华P904808-6.csv', '莫建华P904808-7.csv', '莫建华P904808-8.csv', '莫建华P904808-9.csv', '莫海锋P906550-1.csv', '莫海锋P906550-10.csv', '莫海锋P906550-2.csv', '莫海锋P906550-3.csv', '莫海锋P906550-4.csv', '莫海锋P906550-5.csv', '莫海锋P906550-6.csv', '莫海锋P906550-7.csv', '莫海锋P906550-8.csv', '莫海锋P906550-9.csv', '陆仲波P904145-1.csv', '陆仲波P904145-10.csv', '陆仲波P904145-2.csv', '陆仲波P904145-3.csv', '陆仲波P904145-4.csv', '陆仲波P904145-5.csv', '陆仲波P904145-6.csv', '陆仲波P904145-7.csv', '陆仲波P904145-8.csv', '陆仲波P904145-9.csv', '马丽珊P905960-1.csv', '马丽珊P905960-10.csv', '马丽珊P905960-2.csv', '马丽珊P905960-3.csv', '马丽珊P905960-4.csv', '马丽珊P905960-5.csv', '马丽珊P905960-6.csv', '马丽珊P905960-7.csv', '马丽珊P905960-8.csv', '马丽珊P905960-9.csv', '马伟龙P906651-1.csv', '马伟龙P906651-10.csv', '马伟龙P906651-2.csv', '马伟龙P906651-3.csv', '马伟龙P906651-4.csv', '马伟龙P906651-5.csv', '马伟龙P906651-6.csv', '马伟龙P906651-7.csv', '马伟龙P906651-8.csv', '马伟龙P906651-9.csv', '马秀碧P875903-1.csv', '马秀碧P875903-10.csv', '马秀碧P875903-2.csv', '马秀碧P875903-3.csv', '马秀碧P875903-4.csv', '马秀碧P875903-5.csv', '马秀碧P875903-6.csv', '马秀碧P875903-7.csv', '马秀碧P875903-8.csv', '马秀碧P875903-9.csv', '麦友兴P907803-1.csv', '麦友兴P907803-10.csv', '麦友兴P907803-2.csv', '麦友兴P907803-3.csv', '麦友兴P907803-4.csv', '麦友兴P907803-5.csv', '麦友兴P907803-6.csv', '麦友兴P907803-7.csv', '麦友兴P907803-8.csv', '麦友兴P907803-9.csv', '黄梦羽-1.csv', '黄梦羽-10.csv', '黄梦羽-2.csv', '黄梦羽-3.csv', '黄梦羽-4.csv', '黄梦羽-5.csv', '黄梦羽-6.csv', '黄梦羽-7.csv', '黄梦羽-8.csv', '黄梦羽-9.csv']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 12, 216)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Enable inline plotting\n",
    "data=\"D:/ecg/brugada2/brugada/\"\n",
    "data2=\"D:/ecg/brugada2/右束支阻滞/\"\n",
    "data3=\"D:/ecg/brugada2/正常/\"\n",
    "filelist = os.listdir(data)\n",
    "print(filelist)\n",
    "filelist1=os.listdir(data2)\n",
    "print(filelist1)\n",
    "filelist2=os.listdir(data3)\n",
    "print(filelist2)\n",
    "x_data=[]\n",
    "for i in filelist:\n",
    "    df=np.genfromtxt(data+i,delimiter=',')\n",
    "    x_data.append(df)\n",
    "y1=np.full((len(filelist),1),0)\n",
    "for i in filelist1:\n",
    "    df1=np.genfromtxt(data2+i,delimiter=',')\n",
    "    x_data.append(df1)\n",
    "y2=np.full((len(filelist1),1),1)\n",
    "for i in filelist2:\n",
    "    df2=np.genfromtxt(data3+i,delimiter=',')\n",
    "    x_data.append(df2)   \n",
    "y_data=np.concatenate((np.full((len(filelist),1),0),np.full((len(filelist1),1),1),(np.full((len(filelist2),1),2))),axis=0)\n",
    "x_data=np.array([i for i in x_data])\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FileDeal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "def FileDeal(self,file1,file2,file3):\n",
    "    #取文件下的目录\n",
    "    filelist =os.listdir(file1)\n",
    "    filelist1=os.listdir(file2)\n",
    "    filelist2=os.listdir(file3)\n",
    "    x=np.empty(shape=(0,216))\n",
    "    for s,i in enumerate(filelist):#取数据\n",
    "        df=np.genfromtxt(file1+i,delimiter=',')\n",
    "        x=np.vstack((x,df))\n",
    "        y1=np.full((s+1,1),0)\n",
    "    for s,i in enumerate(filelist1):\n",
    "        df1=np.genfromtxt(file2+i,delimiter=',')\n",
    "        x=np.vstack((x,df1))\n",
    "        y2=np.full((s+1,1),1)\n",
    "    for s,i in enumerate(filelist2):\n",
    "        df2=np.genfromtxt(file3+i,delimiter=',')\n",
    "        x=np.vstack((x,df2))\n",
    "        y3=np.full((s+1,1),2)\n",
    "    y=np.concatenate((y1,y2,y3),axis=0)\n",
    "    wholedata=np.hstack((y,x_data))\n",
    "    return wholedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#建立一个自己的数据集，创建一个将数据处理成DataLoader的类\n",
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, raw_data):\n",
    "        self._signal = torch.FloatTensor(raw_data[:,1:])#信号数据\n",
    "        #self._fea_plus = torch.FloatTensor(raw_data[:, 1:3])\n",
    "        #fea_plus由R前间隔和RR后间隔组成，后面将两个 RR 特征与注意力模块中提取的抽象特征连接起来\n",
    "        self._label = torch.LongTensor(raw_data[:,0])#标签\n",
    "        # self._emd = torch.FloatTensor(emd_data)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def n_insts(self):\n",
    "        ''' Property for dataset size '''\n",
    "        '''数据集大小的属性'''\n",
    "        return len(self._label)\n",
    "\n",
    "    @property\n",
    "    def sig_len(self):\n",
    "        return self._signal.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_insts\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._signal[idx],  self._label[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sublayer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from config import SIG_LEN\n",
    "#SIG_LEN = 241  \n",
    "class MultiHeadAttention(nn.Module):#多头注意力\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):#n_head=4，d_model=64\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head#头的个数为3\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        #三个参数矩阵\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = SDPAttention(temperature=np.power(d_k, 0.5))\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "\n",
    "        mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class SDPAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)#在某一个维度进行softmax\n",
    "        self.BN = nn.BatchNorm1d(SIG_LEN)#sig_len信号长度为241\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attn = self.BN(attn)#对注意力进行批量归一化\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.bmm(attn, v)#输出，value的加权和\n",
    "\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16*4\n",
    "d_model = 64\n",
    "num_layers = 3\n",
    "num_heads = 4\n",
    "class_num = 4\n",
    "d_inner = 512\n",
    "dropout = 0.0\n",
    "warm_steps = 4000\n",
    "fea_num = 7\n",
    "epoch = 100\n",
    "PAD = 0\n",
    "KS = 3\n",
    "Fea_PLUS = 2\n",
    "SIG_LEN = 241"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optim.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A wrapper class for optimizer'''\n",
    "'''用于优化器的包装器类 '''\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "    '''一个简单的学习速率调度的包装类  '''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        '''通过内部优化器消除梯度'''\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "        '''每一步的学习速率调度'''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):#Focal 焦点损失函数\n",
    "    def __init__(self, class_num, alpha=None, gamma=2, average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.average = average\n",
    "\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.ones(class_num, 1)\n",
    "        else:\n",
    "            if isinstance(alpha, torch.Tensor):\n",
    "                self.alpha = alpha\n",
    "            else:\n",
    "                self.alpha = torch.Tensor(alpha)\n",
    "\n",
    "    def forward(self, inputs, targets, device):\n",
    "        N, C = inputs.size()\n",
    "        P = F.softmax(inputs, dim=1)\n",
    "\n",
    "        class_mask = inputs.new_zeros(N, C)\n",
    "        ids = targets.view(-1, 1)\n",
    "        class_mask.scatter_(1, ids, 1.)\n",
    "        if inputs.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.to(device)\n",
    "        alpha = self.alpha[ids.view(-1)]\n",
    "\n",
    "        probs = (P * class_mask).sum(1).view(-1, 1)\n",
    "\n",
    "        log_p = probs.log()\n",
    "\n",
    "        batch_loss = -alpha * torch.pow((1 - probs),self.gamma) * log_p\n",
    "\n",
    "        if self.average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FL = FocalLoss(class_num=5, gamma=2)\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "    N = 4\n",
    "    C = 5\n",
    "    inputs = torch.rand(N, C)\n",
    "    targets = torch.LongTensor(N).random_(C)\n",
    "    inputs_fl = torch.tensor(inputs, requires_grad=True)\n",
    "    targets_fl = torch.tensor(targets)\n",
    "\n",
    "    inputs_ce = torch.tensor(inputs, requires_grad=True)\n",
    "    targets_ce = torch.tensor(targets)\n",
    "    print('----inputs----')\n",
    "    print(inputs)\n",
    "    print('---target-----')\n",
    "    print(targets)\n",
    "\n",
    "    fl_loss = FL(inputs_fl, targets_fl, device = torch.device('cuda:0'))\n",
    "    ce_loss = CE(inputs_ce, targets_ce)\n",
    "    print('ce = {}, fl ={}'.format(ce_loss.item(), fl_loss.item()))\n",
    "    fl_loss.backward()\n",
    "    ce_loss.backward()\n",
    "    #print(inputs_fl.grad.data)\n",
    "    print(inputs_ce.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def plot_roc(all_labels, all_pred):#画出roc曲线\n",
    "    enc = OneHotEncoder()\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_labels = all_labels[:,np.newaxis]\n",
    "    label_h =enc.fit_transform(all_labels)\n",
    "    label_h = label_h.toarray()\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(4):\n",
    "        fpr[i], tpr[i], _ = roc_curve(label_h[:, i], all_pred[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(4)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(4):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= 4\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_h.ravel(), all_pred.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    lw=2\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (area = {0:0.3f})'\n",
    "                   ''.format(roc_auc[\"micro\"]),\n",
    "             color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (area = {0:0.3f})'\n",
    "                   ''.format(roc_auc[\"macro\"]),\n",
    "             color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red'])\n",
    "    c = ['N','S','V','F']\n",
    "    for i, color in zip(range(4), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                 label='ROC curve of class {0} (area = {1:0.3f})'\n",
    "                 ''.format(c[i], roc_auc[i]))\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sublayer import MultiHeadAttention\n",
    "from config import PAD\n",
    "#mlp\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "    '''two-feed-forward-layer模块'''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):#d_hid=d_model=64\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Conv1d(d_in, d_hid, 1)  # position-wise\n",
    "        self.w_2 = nn.Conv1d(d_hid, d_in, 1)  # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        output = x.transpose(1, 2)\n",
    "        output = self.w_2(F.relu(self.w_1(output)))\n",
    "        output = output.transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):#位置编码d_hid=d_model=64\n",
    "    ''' Sinusoid position encoding table '''\n",
    "    '''正弦位置编码表'''\n",
    "\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
    "    #sinusoid相当于pe\n",
    "\n",
    "    if padding_idx is not None:\n",
    "        # zero vector for padding dimension\n",
    "        #填充维数为零的向量\n",
    "        sinusoid_table[padding_idx] = 0.\n",
    "\n",
    "    return torch.FloatTensor(sinusoid_table)#返回位置编码\n",
    "\n",
    "\n",
    "def get_non_pad_mask(seq):\n",
    "    assert seq.dim() == 2#如果seq的维度不等于2，就抛出错误，如果seq的维度等于2就继续执行\n",
    "    return seq.ne(PAD).type(torch.float).unsqueeze(-1)#ne不等于PAD=0位置返回ture,相等的位置返回false\n",
    "\n",
    "\n",
    "def get_subsequent_mask(seq):#mask后面的向量\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    '''用来屏蔽后续信息。'''\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = torch.triu(\n",
    "        torch.ones((len_s, len_s), device=seq.device, dtype=torch.uint8), diagonal=1)#triu上三角矩阵\n",
    "    subsequent_mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "    return subsequent_mask\n",
    "\n",
    "\n",
    "def get_attn_key_pad_mask(seq_k, seq_q):#将pad的位置告诉模型，不让它影响模型的训练\n",
    "    ''' For masking out the padding part of key sequence. '''\n",
    "    '''用于屏蔽密钥序列的填充部分'''\n",
    "\n",
    "    # Expand to fit the shape of key query attention matrix.\n",
    "    #展开拟合键查询注意矩阵的形状\n",
    "    len_q = seq_q.size(1)\n",
    "    padding_mask = seq_k.eq(PAD)\n",
    "    padding_mask = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "    return padding_mask\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "    '''用两层合成'''\n",
    "    \n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(\n",
    "            n_head, d_model, d_k, d_v, dropout=dropout)#多头注意力\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)#MLP网络\n",
    "\n",
    "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
    "        enc_output, enc_slf_attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, mask=slf_attn_mask)#进入多头注意力机制\n",
    "        enc_output *= non_pad_mask#enc_output=enc_output*non_pad_mask\n",
    "\n",
    "        enc_output = self.pos_ffn(enc_output)#进入MLP\n",
    "        enc_output *= non_pad_mask\n",
    "\n",
    "        return enc_output, enc_slf_attn#返回输出和注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from block import get_sinusoid_encoding_table, get_attn_key_pad_mask, get_non_pad_mask, \\\n",
    "    get_subsequent_mask, EncoderLayer\n",
    "from config import PAD, KS, Fea_PLUS\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    ''' A encoder model with self attention mechanism. '''\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_feature,\n",
    "            n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_inner, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        n_position = d_feature + 1#d_feature为signal的长度，241\n",
    "        self.src_word_emb = nn.Conv1d(1, d_model, kernel_size=KS, padding=int((KS - 1) / 2))#KS = 3卷积核的大小为三，d_model=512\n",
    "\n",
    "        self.position_enc = nn.Embedding.from_pretrained(\n",
    "            get_sinusoid_encoding_table(n_position, d_model, padding_idx=0),\n",
    "            freeze=True)\n",
    "\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])#sublayer\n",
    "\n",
    "    def forward(self, src_seq, src_pos):\n",
    "        # -- Prepare masks\n",
    "        non_pad_mask = get_non_pad_mask(src_seq)#未被pad的部分\n",
    "        slf_attn_mask = get_attn_key_pad_mask(seq_k=src_seq, seq_q=src_seq)#将pad的位置告诉模型，不让它影响模型的训练\n",
    "        # -- Forward\n",
    "        enc_output = src_seq.unsqueeze(1)\n",
    "        enc_output = self.src_word_emb(enc_output)\n",
    "        enc_output = enc_output.transpose(1, 2)\n",
    "        enc_output.add_(self.position_enc(src_pos))#加上位置编码\n",
    "\n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(\n",
    "                enc_output,\n",
    "                non_pad_mask=non_pad_mask,\n",
    "                slf_attn_mask=slf_attn_mask)\n",
    "        return enc_output,\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    ''' A sequence to sequence model with attention mechanism. '''\n",
    "    def __init__(\n",
    "            self, device,\n",
    "            d_feature, d_model=512, d_inner=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1,\n",
    "            class_num=5):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(d_feature, n_layers, n_head, d_k, d_v, d_model, d_inner, dropout)\n",
    "        self.device = device\n",
    "\n",
    "        self.linear1_cov = nn.Conv1d(d_feature, 1, kernel_size=1)\n",
    "        self.linear1_linear = nn.Linear(d_model, class_num)\n",
    "        # try different linear style\n",
    "        self.linear2_cov = nn.Conv1d(d_model, 1, kernel_size=1)\n",
    "        self.linear2_linear = nn.Linear(d_feature, class_num)\n",
    "\n",
    "    def forward(self, src_seq):\n",
    "        b, l = src_seq.size()\n",
    "        src_pos = torch.LongTensor(\n",
    "            [list(range(1, l + 1)) for i in range(b)]\n",
    "        )\n",
    "        src_pos = src_pos.to(self.device)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_pos)#进入encoder\n",
    "        dec_output = enc_output\n",
    "        res = self.linear1_cov(dec_output)#卷积\n",
    "        res = res.contiguous().view(res.size()[0], -1)\n",
    "        res = self.linear1_linear(res)#全连接层\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import Transformer\n",
    "from optim import ScheduledOptim\n",
    "from dataset import SignalDataset\n",
    "from config import *\n",
    "from FocalLoss import FocalLoss\n",
    "from entropy import *\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from roc import plot_roc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import os\n",
    "\n",
    "FL = FocalLoss(class_num=4, gamma=1.5, average=False)#损失函数\n",
    "\n",
    "\n",
    "def cal_loss(pred, label, device):#损失\n",
    "    cnt_per_class = np.zeros(4)#一共有四个类别\n",
    "    loss = FL(pred, label, device)\n",
    "\n",
    "    loss = F.cross_entropy(pred, label, reduction='sum')#交叉熵\n",
    "    pred = pred.max(1)[1]#pred.max(1)对维度1取最大值,pred.max(1)[1]返回最大值的位置\n",
    "    n_correct = pred.eq(label).sum().item()#得到预测最大值位置与标签相同的位置，并求和\n",
    "    cnt_per_class = [cnt_per_class[j] + pred.eq(j).sum().item() for j in range(class_num)]\n",
    "    return loss, n_correct, cnt_per_class\n",
    "\n",
    "\n",
    "def cal_statistic(cm):#评价指标\n",
    "    total_pred = cm.sum(0)#对维度0进行求和\n",
    "    total_true = cm.sum(1)#对维度1进行求和\n",
    "    # total_true = np.array([17703,   491,  1357,   159])\n",
    "    # special acc, abnormal inlcuded only\n",
    "    acc_SP = sum([cm[i, i] for i in range(1, class_num)]) / total_pred[0:class_num-1].sum()#混淆矩阵对角线加起来为预测对的，求准确率\n",
    "    pre_i = [cm[i, i] / total_pred[i] for i in range(class_num)]#精准率\n",
    "    rec_i = [cm[i, i] / total_true[i] for i in range(class_num)]#召回率\n",
    "    F1_i = [2 * pre_i[i] * rec_i[i] / (pre_i[i] + rec_i[i]) for i in range(class_num)]#F1分数\n",
    "\n",
    "    pre_i = np.array(pre_i)\n",
    "    rec_i = np.array(rec_i)\n",
    "    F1_i = np.array(F1_i)\n",
    "    #将空值的地方设为0\n",
    "    pre_i[np.isnan(pre_i)] = 0\n",
    "    rec_i[np.isnan(rec_i)] = 0\n",
    "    F1_i[np.isnan(F1_i)] = 0\n",
    "\n",
    "    return acc_SP, list(pre_i), list(rec_i), list(F1_i)\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, device, model, optimizer, total_num):#训练集训练\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    model.train()#训练\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    cnt_per_class = np.zeros(class_num)#分类类别\n",
    "     #valod_loader迭代器，mininterval个更新最小间隔0.5,decs=进度条前缀,leave=false结束时不保存进度条\n",
    "    for batch in tqdm(train_loader, mininterval=0.5, desc='- (Training)  ', leave=False):#加上训练进度条，以便于观察\n",
    "        sig, fea_plus, label, = map(lambda x: x.to(device), batch)#每一批的信号与fea_plus,label\n",
    "        # forward\n",
    "        optimizer.zero_grad()#将梯度清零\n",
    "        pred = model(sig, fea_plus)\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "        all_res.extend(pred.max(1)[1].cpu().numpy())\n",
    "        # backward\n",
    "        loss, n_correct, cnt = cal_loss(pred, label, device)#损失函数\n",
    "        loss.backward()#反向传播梯度\n",
    "        # update\n",
    "        optimizer.step_and_update_lr()#根据梯度优化\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += n_correct\n",
    "        cnt_per_class += cnt\n",
    "    # cm = confusion_matrix(all_labels, all_res)\n",
    "    # print(cm)\n",
    "    # acc_SP, pre_i, rec_i, F1_i = cal_statistic(cm)\n",
    "    # print('acc_SP is : {acc_SP}'.format(acc_SP=acc_SP))\n",
    "    # print('pre_i is : {pre_i}'.format(pre_i=pre_i))\n",
    "    # print('rec_i is : {rec_i}'.format(rec_i=rec_i))\n",
    "    # print('F1_i is : {F1_i}'.format(F1_i=F1_i))\n",
    "    train_loss = total_loss / total_num\n",
    "    train_acc = total_correct / total_num\n",
    "    return train_loss, train_acc, cnt_per_class\n",
    "'''\n",
    "iterable: 迭代器\n",
    "desc: 进度条前缀\n",
    "total: 设置steps, 默认len(iterable)\n",
    "leave: 结束时是否保留进度条,默认True\n",
    "file: 设置在哪里打印进度，默认sys.stderr. 可用file.write(str) 和file.flush() 来写入文件。\n",
    "ncols: 设置进度条的宽度。如果指定则会动态调整去适应该宽度，如果不指定则会适配terminal的最大宽度，可能造成换行。 后面的计数和统计不限宽，如果设为0，则只打印计数和统计信息不显示进度条。(一般设置为70左右比较合适)\n",
    "mininterval: 最小更新间隔,默认0.1s\n",
    "maxinteravl: 最大更新间隔,默认10s\n",
    "miniters: 最小更新间隔,默认10个iteration\n",
    "ascii: 进度条的编码方式\n",
    "position: 进度条打印间隔行数，默认0。一般用于嵌套的进度条\n",
    "'''\n",
    "def eval_epoch(valid_loader, device, model, total_num):#验证集训练\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    cnt_per_class = np.zeros(class_num)\n",
    "    with torch.no_grad():\n",
    "        #valid_loader迭代器，mininterval个更新最小间隔0.5,decs=进度条前缀,leave=false结束时不保存进度条\n",
    "        for batch in tqdm(valid_loader, mininterval=0.5, desc='- (Validation)  ', leave=False):\n",
    "            sig, fea_plus, label, = map(lambda x: x.to(device), batch)#一个batch的数据\n",
    "            # forward\n",
    "            pred = model(sig, fea_plus)  # emd.contiguous().view(len(label), fea_num, -1)#预测值\n",
    "            all_labels.extend(label.cpu().numpy())#标签\n",
    "            all_res.extend(pred.max(1)[1].cpu().numpy())#用来画混淆矩阵\n",
    "            loss, n_correct, cnt = cal_loss(pred, label, device)#损失函数，返回损失和正确\n",
    "\n",
    "            total_loss += loss.item()#所有batch的损失\n",
    "            total_correct += n_correct\n",
    "            cnt_per_class += cnt\n",
    "    cm = confusion_matrix(all_labels, all_res)#混淆矩阵\n",
    "    print(cm)\n",
    "    acc_SP, pre_i, rec_i, F1_i = cal_statistic(cm)\n",
    "    print('acc_SP is : {acc_SP}'.format(acc_SP=acc_SP))#准确率\n",
    "    print('pre_i is : {pre_i}'.format(pre_i=pre_i))#精准率\n",
    "    print('rec_i is : {rec_i}'.format(rec_i=rec_i))#召回率\n",
    "    print('F1_i is : {F1_i}'.format(F1_i=F1_i))#F1分数\n",
    "    valid_loss = total_loss / total_num\n",
    "    valid_acc = total_correct / total_num\n",
    "    return valid_loss, valid_acc, cnt_per_class, sum(rec_i[1:]) * 0.6 + sum(pre_i[1:]) * 0.4\n",
    "\n",
    "\n",
    "def test_epoch(valid_loader, device, model, total_num):#测试集训练\n",
    "    all_labels = []\n",
    "    all_res = []\n",
    "    all_pres = []\n",
    "    all_recs = []\n",
    "    all_pred = []\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    cnt_per_class = np.zeros(class_num)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, mininterval=0.5, desc='- (Validation)  ', leave=False):\n",
    "            sig, fea_plus, label, = map(lambda x: x.to(device), batch)\n",
    "            # forward\n",
    "            pred = model(sig, fea_plus)  # emd.contiguous().view(len(label), fea_num, -1)\n",
    "            #pred是一维数据，label也是一维数据\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_res.extend(pred.max(1)[1].cpu().numpy())\n",
    "            all_pred.extend(pred.cpu().numpy())\n",
    "            loss, n_correct, cnt = cal_loss(pred, label, device)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += n_correct\n",
    "            cnt_per_class += cnt\n",
    "\n",
    "    # np.savetxt('all_pres.txt',all_pres)\n",
    "    # np.savetxt('all_recs.txt', all_recs)\n",
    "    np.savetxt('all_pred.txt',all_pred)#保存\n",
    "    np.savetxt('all_label.txt', all_labels)\n",
    "    all_pred = np.array(all_pred)\n",
    "    plot_roc(all_labels,all_pred)\n",
    "    cm = confusion_matrix(all_labels, all_res)\n",
    "    print(cm)\n",
    "    acc_SP, pre_i, rec_i, F1_i = cal_statistic(cm)\n",
    "    print('acc_SP is : {acc_SP}'.format(acc_SP=acc_SP))\n",
    "    print('pre_i is : {pre_i}'.format(pre_i=pre_i))\n",
    "    print('rec_i is : {rec_i}'.format(rec_i=rec_i))\n",
    "    print('F1_i is : {F1_i}'.format(F1_i=F1_i))\n",
    "    test_acc = total_correct / total_num\n",
    "    print('test_acc is : {test_acc}'.format(test_acc=test_acc))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model_name = 'transform.chkpt'#加载模型\n",
    "    file1=\"D:/ecg/brugada2/brugada/\"\n",
    "    file2=\"D:/ecg/brugada2/右束支阻滞/\"\n",
    "    file3=\"D:/ecg/brugada2/正常/\"\n",
    "    whole_data = FileDeal(file1,file2,file3)\n",
    "\n",
    "    # a, b = SMOTE().fit_sample(whole_data[:, 1:], whole_data[:, 0])\n",
    "    # whole_data = np.c_[b, a]\n",
    "\n",
    "    for r in range(10):\n",
    "        time_start_i = time.time()\n",
    "        raw_train, raw_valid, _, _ = train_test_split(whole_data, list(whole_data[:, 0]), test_size=0.3,\n",
    "                                                          random_state=r,stratify=list(whole_data[:, 0]))\n",
    "        raw_valid, raw_test, _, _ = train_test_split(raw_valid, list(raw_valid[:, 0]), test_size=0.6666,\n",
    "                                                     random_state=r,stratify=list(raw_valid[:, 0]))\n",
    "\n",
    "        if torch.cuda.is_available():#如果有gpu\n",
    "            device = torch.device('cuda:3')#运行设备是gpu\n",
    "        else:\n",
    "            device = torch.device('cpu')#运行设备是cgu\n",
    "            \n",
    "        \n",
    "        #将数据变成自己的数据集以便于DataLoader\n",
    "        train_data = SignalDataset(raw_train)\n",
    "        valid_data = SignalDataset(raw_valid)\n",
    "      \n",
    "        train_loader = DataLoader(dataset=train_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=2,\n",
    "                                  shuffle=True)#训练集\n",
    "        valid_loader = DataLoader(dataset=valid_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  num_workers=2,\n",
    "                                  shuffle=True)#验证集\n",
    "        test_loader = DataLoader(dataset=test_data,\n",
    "                                 batch_size=batch_size,\n",
    "                                 num_workers=2,\n",
    "                                 shuffle=True)#测试集\n",
    "\n",
    "        model = Transformer(device=device, d_feature=train_data.sig_len, d_model=d_model, d_inner=d_inner,\n",
    "                            n_layers=num_layers, n_head=num_heads, d_k=64, d_v=64, dropout=dropout, class_num=class_num)#模型定义\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        optimizer = ScheduledOptim(\n",
    "            Adam(filter(lambda x: x.requires_grad, model.parameters()),\n",
    "                 betas=(0.9, 0.98), eps=1e-09), d_model, warm_steps)#优化\n",
    "        train_accs = []\n",
    "        valid_accs = []\n",
    "        eva_indis = []\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        for epoch_i in range(epoch):#训练次数\n",
    "            print('[ Epoch', epoch_i, ']')\n",
    "            start = time.time()\n",
    "            #训练集效果\n",
    "            train_loss, train_acc, cnt = train_epoch(train_loader, device, model, optimizer, train_data.__len__())#开始训练\n",
    "            print('  - (Training)  loss: {loss: 8.5f}, accuracy: {accu:3.3f} %, '\n",
    "                  'elapse: {elapse:3.3f} min'.format(loss=train_loss, accu=100 * train_acc,\n",
    "                                                     elapse=(time.time() - start) / 60))\n",
    "            print(cnt)\n",
    "            train_accs.append(train_acc)\n",
    "            train_losses.append(train_loss)\n",
    "            start = time.time()\n",
    "            #验证集效果\n",
    "            valid_loss, valid_acc, cnt, eva_indi = eval_epoch(valid_loader, device, model, valid_data.__len__())\n",
    "            print('  - (Validation)  loss: {loss: 8.5f}, accuracy: {accu:3.3f} %, '\n",
    "                  'elapse: {elapse:3.3f} min'.format(loss=valid_loss, accu=100 * valid_acc,\n",
    "                                                     elapse=(time.time() - start) / 60))\n",
    "            print(cnt)\n",
    "            valid_accs.append(valid_acc)\n",
    "            eva_indis.append(eva_indi)\n",
    "            valid_losses.append(valid_loss)\n",
    "            model_state_dict = model.state_dict()\n",
    "\n",
    "            checkpoint = {\n",
    "                'model': model_state_dict,\n",
    "                'config_file': 'config',\n",
    "                'epoch': epoch_i}#模型保存\n",
    "\n",
    "            if eva_indi >= max(eva_indis):\n",
    "                torch.save(checkpoint, str(r)+model_name)\n",
    "                print('    - [Info] The checkpoint file has been updated.')\n",
    "\n",
    "        print('——ALL DONE!——')\n",
    "        time_consume = (time.time() - time_start_i)\n",
    "        print('total ' + str(time_consume) + 'seconds')\n",
    "        plt.plot(valid_losses)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylim([0.0, 0.5])\n",
    "        plt.ylabel('valid loss')\n",
    "        plt.title('loss change curve')\n",
    "        plt.show()\n",
    "        # pre = time.strft  ime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n",
    "        # config = [batch_size, d_model, num_layers, num_heads, class_num, d_inner, dropout]\n",
    "        # np.save(pre + '_config.npy', config)\n",
    "        # np.save(pre + '_train_accs.npy', train_accs)\n",
    "        # np.save(pre + '_valid_accs.npy', valid_accs)\n",
    "        # np.save(pre + '_train_losses.npy', train_losses)\n",
    "        # np.save(pre + '_valid_losses.npy', valid_losses)\n",
    "        #测试集效果\n",
    "        test_model_name = str(r) + model_name\n",
    "        model = Transformer(device=device, d_feature=test_data.sig_len, d_model=d_model, d_inner=d_inner,\n",
    "                            n_layers=num_layers, n_head=num_heads, d_k=64, d_v=64, dropout=dropout,\n",
    "                            class_num=class_num)\n",
    "        chkpoint = torch.load(test_model_name, map_location='cuda:3')\n",
    "        model.load_state_dict(chkpoint['model'])\n",
    "        model = model.to(device)\n",
    "        test_epoch(test_loader, device, model, test_data.__len__())\n",
    "\n",
    "    # models = []\n",
    "    # for r in range(10):\n",
    "    #     test_model_name = str(r) + model_name\n",
    "    #     model = Transformer(device=device, d_feature=test_data.sig_len, d_model=d_model, d_inner=d_inner,\n",
    "    #                         n_layers=num_layers, n_head=num_heads, d_k=64, d_v=64, dropout=dropout,\n",
    "    #                         class_num=class_num)\n",
    "    #     chkpoint = torch.load(test_model_name)\n",
    "    #     model.load_state_dict(chkpoint['model'])\n",
    "    #     model = model.to(device)\n",
    "    #     models.append(model)\n",
    "    # voting_epoch(test_loader, device, models, test_data.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
